# -*- coding: utf-8 -*-
"""Membangun DL Model dengan TensorFlow

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gFIft4AhfPI-eEqWPFPv-JJ6CmPdCi7M

# Implementasi Neural Networks pada **TensorFlow**

## meng-*load* data dan observasi data

1. Mengimpor library yang diperlukan:
* `pandas`: Library untuk manipulasi dan analisis data, biasanya digunakan untuk membaca dan menulis data dalam berbagai format seperti CSV, Excel, dan lainnya.
* `train_test_split`: Fungsi dari scikit-learn yang digunakan untuk membagi data menjadi set pelatihan dan pengujian.
* `StandardScaler`: Fungsi dari scikit-learn yang digunakan untuk penskalaan fitur dengan menghapus rata-rata dan menskalakan ke varians satuan.
* `Normalizer`: Fungsi dari scikit-learn yang digunakan untuk melakukan normalisasi fitur.
* `ColumnTransformer`: Fungsi dari scikit-learn yang digunakan untuk menggabungkan beberapa fitur ekstraksi mekanisme atau algoritma pra-pemrosesan berbeda menjadi satu transformer.
* `Sequential, InputLayer, Dense, `dan` Adam` dari TensorFlow: Komponen untuk membangun model Deep Learning.
* `plot_model` dari TensorFlow dan Keras untuk membuat visualisasi model.
* `matplotlib.pyplot` untuk membuat plot dan visualisasi.

2. Membaca dataset 'Life Expectancy Data' menggunakan pandas, yang disimpan dalam variabel `dataset`. Lokasi file dataset ini di `/content/drive/MyDrive/workshop-python/Life Expectancy Data.csv`.

3. Menghapus kolom 'Country' dari dataset menggunakan metode `.drop()`. Parameter `axis=1` menunjukkan bahwa kolom tersebut dihapus (jika `axis=0`, itu akan menghapus baris). Dataset yang sudah diubah kemudian disimpan kembali ke variabel `dataset`.

4. Menampilkan 5 baris pertama dari dataset dengan menggunakan metode `.head()`. Ini memberikan gambaran awal tentang data yang akan digunakan.

5. Menampilkan statistik deskriptif dari dataset dengan menggunakan metode .`describe()`. Statistik ini mencakup jumlah, rata-rata, standar deviasi, nilai minimum, kuartil bawah, median, kuartil atas, dan nilai maksimum untuk setiap kolom numerik dalam dataset. Hal ini berguna untuk memahami distribusi dan karakteristik data.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import Normalizer
from sklearn.compose import ColumnTransformer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import plot_model
import matplotlib.pyplot as plt

dataset = pd.read_csv('/content/drive/MyDrive/workshop-python/Life Expectancy Data.csv')
dataset = dataset.drop(['Country'], axis = 1)

print(dataset.head())
print(dataset.describe())

"""Kode ini berfungsi untuk memisahkan dataset menjadi fitur (features) dan label (labels). Berikut adalah penjelasan singkat mengenai bagian ini:

1. `labels = dataset.iloc[:, -1]`: Menggunakan metode `.iloc[]` pada objek DataFrame dataset untuk memilih semua baris (ditunjukkan oleh :) dan kolom terakhir (ditunjukkan oleh `-1`). Hasilnya disimpan dalam variabel `labels`. Dalam konteks ini, label adalah kolom yang ingin kita prediksi, yaitu harapan hidup.

2. `features = dataset.iloc[:, 0:-1]`: Sama seperti sebelumnya, kita menggunakan metode `.iloc[]` pada objek DataFrame `dataset`. Kali ini, kita memilih semua baris (ditunjukkan oleh `:`) dan kolom mulai dari awal (ditunjukkan oleh `0`) hingga kolom kedua terakhir (ditunjukkan oleh `-1`, yang secara efektif menghilangkan kolom terakhir). Hasilnya disimpan dalam variabel `features`. Fitur adalah kolom yang digunakan sebagai variabel independen untuk memprediksi label (harapan hidup).

3. `print(labels)`: Menampilkan label yang telah dipisahkan dari dataset.

4. `print(features)`: Menampilkan fitur yang telah dipisahkan dari dataset.
"""

labels = dataset.iloc[:, -1] #memilih semua row (:), dan mengakses kolum terakhir (-1)
features = dataset.iloc[:, 0:-1] #memilih semua row (:), dan akses column dari awal (0) hingga kolum akhir (-1)
print(labels)
print(features)

"""## Data Preprocessing

Kode ini menjelaskan langkah-langkah pra-pemrosesan data sebelum memasukkannya ke dalam model Deep Learning. Berikut adalah penjelasan singkat mengenai bagian ini:

1. `features = pd.get_dummies(dataset)`: Fungsi `pd.get_dummies()` digunakan untuk mengkonversi variabel kategorikal menjadi variabel dummy/indikator. Namun, dalam konteks kode ini, baris ini tampaknya tidak diperlukan karena dataset yang digunakan tidak memiliki variabel kategorikal dan `dataset` seharusnya digantikan dengan `features`.

2. `features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.20, random_state=23)`: Fungsi `train_test_split()` dari scikit-learn digunakan untuk membagi dataset (fitur dan label) menjadi set pelatihan dan pengujian. Parameter `test_size = 0.20` menunjukkan bahwa 20% dari data akan digunakan untuk pengujian, dan sisanya untuk pelatihan. Parameter `random_state = 23` digunakan untuk mengatur seed generator bilangan acak, yang memastikan hasil yang sama setiap kali kode dijalankan.

3. `numerical_features = features.select_dtypes(include=['float64', 'int64'])`: Metode `.select_dtypes()` digunakan untuk memilih kolom dalam DataFrame berdasarkan tipe data tertentu. Dalam hal ini, kita memilih kolom dengan tipe data 'float64' dan 'int64' dan menyimpannya dalam variabel `numerical_features`. Ini akan digunakan nantinya untuk penskalaan fitur numerik.

4. `numerical_columns = numerical_features.columns:` Mengakses nama kolom dari `numerical_features` dan menyimpannya dalam variabel `numerical_columns`. Ini akan digunakan sebagai input untuk `ColumnTransformer`.

5. `ct = ColumnTransformer([("only numeric", StandardScaler(), numerical_columns)], remainder='passthrough')`: Membuat objek `ColumnTransformer` yang digunakan untuk menggabungkan beberapa fitur ekstraksi mekanisme atau algoritma pra-pemrosesan dalam satu transformer. Dalam hal ini, kita menggunakan `StandardScaler()` untuk penskalaan fitur numerik yang diidentifikasi sebelumnya. Parameter remain`der = 'passthrough'` menunjukkan bahwa kolom yang tidak ditransformasikan harus disertakan dalam output akhir tanpa perubahan.
"""

features = pd.get_dummies(dataset)
features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.20, random_state=23)

numerical_features = features.select_dtypes(include=['float64', 'int64'])
numerical_columns = numerical_features.columns
 
ct = ColumnTransformer([("only numeric", StandardScaler(), numerical_columns)], remainder='passthrough')

"""Kode ini menjelaskan langkah penskalaan fitur dalam set pelatihan dan pengujian menggunakan objek ColumnTransformer yang telah dibuat sebelumnya. Berikut adalah penjelasan singkat mengenai bagian ini:

1. `features_train_scaled = ct.fit_transform(features_train)`: Metode `.fit_transform()` pada objek `ColumnTransformer (ct)` digunakan untuk melakukan dua langkah sekaligus: (1) "fit" atau menyesuaikan penskalaan pada set pelatihan (fitur numerik saja), dan (2) mengubah fitur pelatihan sesuai dengan penskalaan yang telah ditentukan. Hasil penskalaan fitur pelatihan disimpan dalam variabel `features_train_scaled`.

2. `features_test_scaled = ct.fit_transform(features_test)`: Metode `.fit_transform()` juga digunakan pada set pengujian. Namun, penting untuk dicatat bahwa seharusnya kita menggunakan metode `.transform()` daripada `.fit_transform()` pada set pengujian. Ini karena kita ingin menggunakan parameter penskalaan yang sama yang telah "fit" pada set pelatihan untuk menghindari kebocoran informasi dari set pengujian ke set pelatihan. Jadi, kode yang benar adalah: `features_test_scaled = ct.transform(features_test)`.
"""

features_train_scaled = ct.fit_transform(features_train)
features_test_scaled = ct.fit_transform(features_test)

"""## Membuat Model

Kode ini menjelaskan bagian awal dari pembuatan model Deep Learning menggunakan TensorFlow. Berikut adalah penjelasan singkat mengenai bagian ini:

1. `model_saya = Sequential()`: Membuat objek model Sequential dari TensorFlow dan menyimpannya dalam variabel `model_saya`. Model Sequential adalah tumpukan linear lapisan dalam model Deep Learning, di mana output satu lapisan menjadi input lapisan berikutnya.

2. `input_saya = InputLayer(input_shape = (features.shape[1], ))`: Membuat objek InputLayer dari TensorFlow dan menyimpannya dalam variabel `input_saya`. InputLayer bertindak sebagai lapisan input untuk model Sequential. Parameter `input_shape` menentukan bentuk input yang akan diterima oleh lapisan ini. Dalam hal ini, bentuk input adalah jumlah kolom dalam fitur (ditunjukkan oleh `features.shape[1]`). Tanda koma dan tanda kurung di akhir `(features.shape[1], )` diperlukan untuk menunjukkan bahwa bentuknya adalah tuple dengan satu elemen.

3. `model_saya.add(input_saya)`: Menambahkan lapisan input yang telah dibuat (`input_saya`) ke model Sequential (`model_saya`). Ini menetapkan lapisan input sebagai lapisan pertama dalam arsitektur model Deep Learning.
"""

model_saya = Sequential()
input_saya = InputLayer(input_shape = (features.shape[1], ))

model_saya.add(input_saya)

"""Kode ini menambahkan lapisan tersembunyi dan lapisan output ke model Deep Learning yang telah diinisialisasi sebelumnya, serta menampilkan ringkasan model. Berikut adalah penjelasan singkat mengenai bagian ini:

1. `model_saya.add(Dense(64, activation = "relu"))`: Menambahkan lapisan Dense (Fully Connected) ke model Sequential (`model_saya`). Lapisan ini memiliki 64 unit (neuron) dan menggunakan fungsi aktivasi ReLU (Rectified Linear Unit). Fungsi aktivasi ReLU adalah fungsi yang populer dalam Deep Learning karena kemampuannya untuk mengatasi masalah vanishing gradient dan efisiensi komputasi. Lapisan ini bertindak sebagai lapisan tersembunyi dalam arsitektur model.

2. `model_saya.add(Dense(1))`: Menambahkan lapisan Dense lainnya ke model Sequential (`model_saya`). Lapisan ini memiliki 1 unit (neuron) dan tidak menggunakan fungsi aktivasi. Lapisan ini berfungsi sebagai lapisan output dalam arsitektur model. Karena masalah ini adalah masalah regresi (memprediksi nilai numerik), lapisan output hanya perlu satu neuron untuk menghasilkan prediksi tunggal.

3. `model_saya.summary()`: Menampilkan ringkasan arsitektur model Deep Learning yang telah dibuat. Ringkasan ini mencakup informasi tentang lapisan yang ada dalam model, bentuk output dari setiap lapisan, dan jumlah parameter yang dapat dilatih dalam model.
"""

model_saya.add(Dense(64, activation = "relu"))
model_saya.add(Dense(1))

model_saya.summary()

"""## inisialisasi optimasi dan *compiling* modelnya

Kode ini membuat objek optimizer Adam dengan learning rate yang telah ditentukan. Berikut adalah penjelasan singkat mengenai bagian ini:

`opt = Adam(learning_rate = 0.01)`: Membuat objek optimizer Adam dari TensorFlow dan menyimpannya dalam variabel `opt`. Parameter `learning_rate = 0.01` menentukan kecepatan pembelajaran yang akan digunakan oleh optimizer saat melatih model. Adam (Adaptive Moment Estimation) adalah algoritma optimasi yang populer dalam Deep Learning karena efisiensi dan kinerjanya yang baik.
"""

opt = Adam(learning_rate = 0.01)

"""Kode ini mengkompilasi model Deep Learning dengan menentukan fungsi kerugian (loss function), metrik evaluasi, dan optimizer yang akan digunakan selama proses pelatihan. Berikut adalah penjelasan singkat mengenai bagian ini:

`model_saya.compile(loss = 'mse', metrics = ['mae'], optimizer = opt)`:

* `loss = 'mse'`: Menentukan fungsi kerugian yang digunakan dalam proses pelatihan sebagai Mean Squared Error (MSE). MSE adalah metrik yang menghitung rata-rata kuadrat perbedaan antara nilai yang sebenarnya dan nilai yang diprediksi. Ini adalah pilihan umum untuk masalah regresi.

* `metrics = ['mae']`: Menentukan metrik evaluasi yang akan digunakan untuk mengukur kinerja model selama proses pelatihan. Dalam hal ini, kita menggunakan Mean Absolute Error (MAE) sebagai metrik evaluasi. MAE menghitung rata-rata absolut perbedaan antara nilai yang sebenarnya dan nilai yang diprediksi.

* `optimizer = opt`: Menetapkan optimizer yang akan digunakan selama proses pelatihan. Dalam kasus ini, kita menggunakan objek optimizer Adam yang telah dibuat sebelumnya dengan learning rate 0.01 (variabel `opt`).
"""

model_saya.compile(loss = 'mse', metrics = ['mae'], optimizer = opt)

"""## Fit dan evaluasi model

Kode ini melatih model Deep Learning menggunakan fitur yang telah diolah dan disesuaikan serta label yang sesuai. Berikut adalah penjelasan singkat mengenai bagian ini:

`model_saya.fit(features_train_scaled, labels_train, epochs = 40, batch_size = 1, verbose = 1)`:

* `features_train_scaled`: Fitur pelatihan yang telah diolah dan diskalakan sebelumnya. Ini akan digunakan sebagai input untuk melatih model.

* `labels_train`: Label pelatihan yang sesuai dengan fitur pelatihan. Ini akan digunakan sebagai target output saat melatih model.

* `epochs = 40`: Jumlah kali seluruh dataset pelatihan melewati model selama proses pelatihan. Satu epoch berarti satu kali perhitungan maju dan mundur untuk semua sampel pelatihan.

* `batch_size = 1`: Jumlah sampel yang digunakan untuk mengupdate model dalam setiap iterasi. Dalam hal ini, kita menggunakan Stochastic Gradient Descent (SGD) dengan batch size 1, yang berarti model akan diperbarui setelah setiap sampel pelatihan.

* `verbose = 1`: Tingkat kebisingan (verbose level) selama proses pelatihan. Nilai 1 berarti bahwa model akan mencetak log pelatihan yang mencakup informasi tentang loss dan metrik evaluasi setelah setiap epoch.
"""

model_saya.fit(features_train_scaled, labels_train, epochs = 40, batch_size = 1, verbose = 1)

"""Kode ini mengevaluasi kinerja model Deep Learning yang telah dilatih menggunakan dataset pengujian dan mencetak hasilnya. Berikut adalah penjelasan singkat mengenai bagian ini:

1. `res_mse, res_mae = model_saya.evaluate(features_test_scaled, labels_test, verbose = 0)`: Metode `evaluate()` pada objek model TensorFlow (`model_saya`) digunakan untuk mengevaluasi kinerja model pada dataset pengujian. Parameter `features_test_scaled` adalah fitur pengujian yang telah diskalakan, dan `labels_test` adalah label pengujian yang sesuai. Parameter `verbose = 0` menunjukkan bahwa tidak ada output yang akan dicetak selama proses evaluasi. Fungsi `evaluate()` mengembalikan nilai loss (dalam kasus ini, MSE) dan metrik evaluasi yang ditentukan saat mengkompilasi model (dalam kasus ini, MAE). Nilai ini disimpan dalam variabel `res_mse` dan `res_mae`.

2. `print(res_mse, res_mae)`: Mencetak nilai MSE dan MAE yang dihasilkan dari evaluasi model. Ini memberikan gambaran tentang seberapa baik model Deep Learning yang telah dilatih dalam memprediksi label pada dataset pengujian. Nilai yang lebih rendah untuk MSE dan MAE menunjukkan kinerja yang lebih baik.
"""

res_mse, res_mae = model_saya.evaluate(features_test_scaled, labels_test, verbose = 0)
print(res_mse, res_mae)

"""Hasil yang diberikan (contohnya berikut: 2.010579824447632 untuk MSE dan 0.8976656794548035 untuk MAE) merupakan ukuran kesalahan prediksi model pada data uji. Semakin kecil nilai MSE dan MAE, semakin baik kinerja model dalam memprediksi umur harapan hidup. Di sini, MAE sekitar 0.898 mengindikasikan bahwa rata-rata kesalahan prediksi model adalah sekitar 0.898 tahun.

Kode ini menciptakan sebuah plot yang memvisualisasikan perbandingan antara nilai sebenarnya (actual values) dan prediksi yang dihasilkan oleh model Deep Learning (`model_saya`) untuk data uji. Berikut adalah penjelasan kode tersebut:

1. Memprediksi nilai target pada data uji yang telah dinormalisasi menggunakan `model_saya.predict(features_test_scaled)`, dan menyimpan hasilnya dalam variabel `predictions`.

2. Menghitung perbedaan absolut antara nilai sebenarnya (`labels_test`) dan prediksi dengan mengurangi nilai prediksi dari nilai sebenarnya dan mengambil nilai absolutnya. Kemudian menyimpan hasilnya dalam variabel `diff`.

3. Membuat scatter plot dengan nilai sebenarnya pada sumbu x dan prediksi pada sumbu y. Warna titik dalam plot ini diatur berdasarkan perbedaan absolut antara nilai sebenarnya dan prediksi (variabel `diff`), menggunakan colormap 'viridis'.

4. Memberi label pada sumbu x sebagai 'Nilai Sebenarnya' dan sumbu y sebagai 'Prediksi', serta memberikan judul plot 'Perbandingan Nilai Sebenarnya dan Prediksi'.

5. Menemukan nilai minimum dan maksimum antara nilai sebenarnya dan prediksi untuk digunakan sebagai titik awal dan akhir garis merah.

6. Menggambar garis merah diagonal pada plot yang merepresentasikan idealnya, di mana nilai sebenarnya sama dengan nilai prediksi. Garis ini akan membantu Anda melihat seberapa baik model Anda dalam memprediksi nilai sebenarnya.

7. Menambahkan colorbar yang menunjukkan skala perbedaan absolut antara nilai sebenarnya dan prediksi, dengan label 'Perbedaan Absolut'.

8. Menampilkan plot dengan `plt.show()`.
"""

predictions = model_saya.predict(features_test_scaled)

diff = abs(labels_test - predictions.flatten())

plt.scatter(labels_test, predictions, c=diff, cmap='viridis')
plt.xlabel('Nilai Sebenarnya')
plt.ylabel('Prediksi')
plt.title('Perbandingan Nilai Sebenarnya dan Prediksi')

min_val = min(min(labels_test), min(predictions.flatten()))
max_val = max(max(labels_test), max(predictions.flatten()))

plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='-', lw=2)

plt.colorbar().set_label('Perbedaan Absolut')
plt.show()

"""1. Banyak titik yang lebih ke kiri dari garis:
Ini berarti model cenderung memprediksi nilai yang lebih tinggi daripada nilai sebenarnya. Dalam hal ini, prediksi lebih besar daripada yang seharusnya.

2. Banyak titik yang lebih ke kanan dari garis:
Ini berarti model cenderung memprediksi nilai yang lebih rendah daripada nilai sebenarnya. Dalam hal ini, prediksi lebih kecil daripada yang seharusnya.

3. Banyak titik yang lebih ke atas dari garis:
Ini sebenarnya sama dengan titik-titik yang lebih ke kanan dari garis. Hal ini menunjukkan bahwa model memprediksi nilai yang lebih rendah daripada nilai sebenarnya.

4. Banyak titik yang lebih ke bawah dari garis:
Ini sebenarnya sama dengan titik-titik yang lebih ke kiri dari garis. Hal ini menunjukkan bahwa model memprediksi nilai yang lebih tinggi daripada nilai sebenarnya.

Garis diagonal merah pada grafik menunjukkan tempat di mana nilai sebenarnya sama dengan prediksi. Jadi, jika titik-titik di grafik lebih dekat ke garis ini, berarti prediksi yang dibuat oleh model lebih akurat.

Warna titik-titik pada grafik menunjukkan perbedaan absolut antara nilai sebenarnya dan prediksi. Titik-titik dengan warna yang lebih gelap menunjukkan perbedaan yang lebih kecil (prediksi lebih akurat), sementara titik-titik dengan warna yang lebih terang menunjukkan perbedaan yang lebih besar (prediksi kurang akurat).
"""